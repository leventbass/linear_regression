{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Qh7SBCLFy07"
   },
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "# Linear Regression from Scratch with NumPy\n",
    "---\n",
    "\n",
    "Welcome to the first post of the **Implementing Machine Learning Algorithms with NumPy** series in which I'll try to show how one can implement some machine learning algorithms with `numpy` package only. \n",
    "\n",
    "Of course, we will use other useful packages such as `matplotlib`, ` seaborn` and etc. However, the use of other packages may only be limited to data visualization, data manipulation and/or loading datasets (e.g. `sklearn.datasets`) such that we won't take any shortcuts while writing the actual code for machine learning models.\n",
    "\n",
    "To sum it up, we will be implementing machine learning algorithms from scratch! Isn't that exciting and little bit overwhelming at the same time? Did I mention that it is super fun as well? The first algorithm that we will tackle is linear regression. Since it is the \"hello world\" algorithm of the machine learning universe, it will be pretty easy to implement it with NumPy. Let's start right away!\n",
    "\n",
    "## Linear Regression Intuition\n",
    "---\n",
    "\n",
    "\n",
    "Before we write the code for implementation of linear regression, first we need to understand what linear regression is. There are many useful resources out there that makes it quite easy to understand the concept behind regression and particularly linear regression so, I won't be going into much detail here. \n",
    "\n",
    "Linear regression is used to make some sense of the data we have at hand by unearthing the relation between target values and features of the data. When we know this relation, we can make predictions about the data that we haven't seen before, in other words, we can infer the target value from feature values. Let's exemplify this: \n",
    "\n",
    "Suppose we want to understand how a company X decides what to pay to its employees. There may be so many factors that go into that decision so, we go around and ask most of the employees who work there. After a lot of prying and sneaking around, it turns out that, some of them earn a lot because they have been working at the company X for quite some time, some of them earn higher than most simply because they get along really well with the boss and some earn higher because of their qualifications and talent. These three indicators seem to be the major ones so we'll be using these three only. Now, with the information we have gathered, we want to understand the underlying relation between these factors and the salary that is paid to the employees currently. We come up with this oversimpflied equation:\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 16px; line-height:3.2\">\n",
    "    \n",
    "**SALARY** = (? x _**Qualifications**_) + (? x **_Length of Service_**) + (? x _**Buttering up the Boss**_)\n",
    "    \n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "    \n",
    "We can see from the equation above that the salary is affected by 3 attributes. These attributes, also called **features**, affect the salary according to their own weight which is depicted in the equation as question marks simply because we don't actually know what these weights are. \n",
    "\n",
    "Now, let's imagine what would happen if we know these weights exactly. Then, if we have an employee whose salary we don't know, we can use the features (qualifications, length of service etc.) of the employee to predict the employee's salary, that is, we would understand how these features and the target value (salary) are related. \n",
    "\n",
    "Turns out, linear regression is used to do exactly that! It is used to get a good estimate of these weights so that they can be used to predict the target value of unseen data. In machine learning world, these weights are often called **parameters**, hence from now on, we'll adopt that term here as well. \n",
    "\n",
    "\n",
    "## Gradient Descent Algorithm\n",
    "---\n",
    "\n",
    "\n",
    "Now that we know **what** linear regression is practically, we can come to the **how** part. How does this algorithm work? How can we figure out these parameters for linear regression? In machine learning, there is another famous algorithm called **gradient descent** that is widely used, not only for estimating the parameters for linear regression but for other optimization problems as well. In gradient descent algorithm, parameters of the model is changed iteratively at each step starting with the initial values of the parameters. \n",
    "\n",
    "To remind us once more, parameters (weights) are the numerical values that determine how much each feature affects the target value. We want to know these parameter values exactly, but in real life this is not possible because there may be  other features (hence, parameters of those features as well) affecting the target value. However, we want them to predict the target value as close as possible to the actual value. Since, the question marks in the above equation represent the parameter values, we can replace them with values like this:\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 16px; line-height:3.2\">\n",
    "\n",
    "**SALARY** = (1000 x _**Qualifications**_) + (200 x **_Length of Service_**) + (500 x _**Buttering up the Boss**_)\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "Here, it is obvious that qualifications feature affects salary more than the other features, because its parameter value is higher than the rest. Keep in mind that we have chosen these parameter values intuitively and we will be using them as our initial parameter values, yet these initial values will change at every step of the algorithm towards their optimal values.\n",
    "\n",
    "Going along with our analogy, suppose we have an initial estimate for the parameters of these features and we went around and asked these questions to the first employee we could find:\n",
    "\n",
    " * For how long have you been working here?\n",
    " * What are your qualifications for your position?\n",
    " * How do you get along with your boss? (Does your boss seem to like or dislike you?) \n",
    "\n",
    "For the first question, we told the employee that we would accept an answer in years (1 year, 2 years, 5 years etc.). For the second question, we told the employee that the answer would be any number from 1 to 10 (1 being the least qualified and 10 the most). For the last question, the answer would be a number from -5 to 5. Here, minus represents the negativity of the relationship between the employee and the boss. Therefore, -5 means that the boss quite dislikes the employee, 0 could mean that the boss doesn't even know the employee and/or there is no interaction between the two and +5 means that two of them get along just great. \n",
    "\n",
    "When we asked the employee these questions, these were the answers we got:\n",
    "\n",
    "* I've have been working here for 10 years.\n",
    "* I can honestly say that I'm overqualified for this job. So I would give it a 9.\n",
    "* My boss seems to hate me. Whenever I'm around I can see the hatred in his eyes. So I would give it a -4.\n",
    "\n",
    "Remember that we want to predict the salary based the parameters we have chosen and the answers we've got from the employee. After predicting what the salary would be based on only these answers, we ask the employee what her actual salary is. The difference between the predicted and actual value determines how successful our estimates for these parameters (weights) are. Gradient descent algorithm's job is to make this difference (predicted - actual) as small as possible. Let's go ahead and call this difference **error**, since it represents how much off the actual value is from the predicted value. Now, let's plug the numbers we get from the first employee's answers into our equation:\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 16px; line-height:3.2\">\n",
    "\n",
    "**SALARY** = (1000 x 10) + (200 x 9) + (500 x -4)\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "Hence, this shows that our prediction for the salary is:\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 16px; line-height:3.2\">\n",
    "\n",
    "**SALARY**<sub>predicted</sub> = 12800\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.5\">\n",
    "\n",
    "Now, after getting the actual salary from the employee, we calculate the error between the actual and predicted value:\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 16px; line-height:2.2\">\n",
    "\n",
    "**SALARY**<sub>actual</sub> = 9800\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 16px; line-height:3.2\">\n",
    "\n",
    "**Error** = **SALARY**<sub>predicted</sub> - **SALARY**<sub>actual</sub> = 12800 - 9800 = 3000\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "We see that our error is **3000**, we want to make this error as small as possible by tweaking the parameter values appropriately. But how do we do that? How do we decide what is the correct way of changing the parameter values? Obviously, we can make guesses intuitively and change the parameter values (increase or decrease) to make the error small enough. However, this won't be practical if we have one hundred features and not only three. One hundred features mean one hundred parameter values, remember? Obviously, we have to find a better way than this.\n",
    "\n",
    "Moreover, there is another factor to consider. This error cannot represent only one employee, in other words, we cannot only change the parameter values for one employee since we want this model to be representative for all the employees who work at company X. We have to get the answer from each employee and plug those values into the equation, find the error and change the parameters accordingly. \n",
    "\n",
    "The error function that we have used here (Error = Predicted - Actual) is one of the most basic functions in machine learning which has its certain limitations. Therefore, while implemeting linear regression, we will use a more sophisticated version called **sum of squared errors (SSE)**, which is simply the the sum of square differences between the actual and predicted values.\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "    \n",
    "Now, a quick change of notation is in order. The term **cost** is often used a lot instead of the term erro' and we'll use that here as well, since it costs us to miss the actual value by an amount of predicted-actual. If predicted value was equal to the actual value, the cost would be zero. Therefore, a cost function is used as a measure of how wrong the model is in terms of its ability to estimate the relationship between feature values and target values.\n",
    "\n",
    "After establishing which cost function to use, we can now move on. The whole point of gradient descent algorithm is to minimize the cost function. When we minimize the cost function, we are actually ensuring the possible lowest error while increasing the accuracy of our model. We go over our dataset iteratively while updating the parameters at each step. Back to our analogy, remember that we had three parameters (qualifications, length of service, buttering up the boss) that we wanted to change in the direction that minimized the cost function. So checking each data point in our dataset basically means asking each employee who works at company X those 3 questions we convised, using plugging the values we extract from answers into the cost function and deciding into which direction the next step should be taken to minimize the cost function.\n",
    "\n",
    "Now, how do we decide which direction we should move towards to make the total cost lower? Calculus comes to our help here. When we want to minimize a function we take the derivative of the function with respect to a variable and use that derivative to decide which direction to go. In our analogy, the parameters that we have chosen are actually the **variables** of our cost function because the cost function varies as each parameter varies (variable, duh). We have to take the derivative with respect to each parameter and update the parameters using those derivative values. In the picture below, we can see the graph of the cost function against just one parameter (Length of Service). Now when we calculate the partial derivative of the cost function with respect to this parameter only, we get the direction we need to move towards for this parameter, in order to reach the local minima whose slope equals to 0.\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:1.2\">\n",
    "\n",
    "<img src=\"img/cost_function.png\" width=400 height=200 > <br> <br>\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "When we take the derivative with respect to each parameter and find the direction we need to move towards, we update each parameter simultaneously:\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 16px; line-height:3.2\">\n",
    "\n",
    "**Length of Service**<sub>updated</sub> = **Length of Service**<sub>old</sub> - (*Learning Rate* x **Partial Derivative w.r.t. Length of Service**)\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "This update rule is applied to all of the parameters using their partial derivatives correspondingly. When we apply update rule once, it means one iteration. Here, **learning rate**, also called learning step, is the amount that the parameters are updated during learning. Learning rate is a configurable hyperparameter, often chosen between 0.0 and 1.0, that defines the rate or speed at which the model learns. If it's high, the model learns quickly, however it's too high, we might miss the optimal value during learning because we might have taken a really big step. If the learning rate is too low, then the model will take a lot of time to converge to the lowest cost function value. Obviously, tuning this hyperparameter is of great importance in machine learning.\n",
    "\n",
    "So one iteration means asking each employee those three questions only once (or going over the dataset once) and updating the parameter values accordingly. After iterating the dataset many times, iterating stops when we reach a point where the cost is low enough for us to decide that we can stop the algorithm and use the parameter values that were updated up until now. Then, we can use those **optimized** values to predict new target values for new feature values. What do we mean by optimized here? Well, now we have found parameter values for our three features, so that they can predict new target values with lowest possible error. Hence, we optimized those parameters in our model. This is where learning of the machine learning happens indeed. We **learn** the parameters that minimizes our cost function.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fOVZo_QaP4eX"
   },
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "    \n",
    "## Linear Regression Implementation (Finally!)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-X9lP0wwR5u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3_6cl5YwUMw"
   },
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "First things first, we start by importing necessary libraries to help us along the way. As I have mentioned before, we won't be using any packages that will give us already implemented algorithm models such as `sklearn.linear_model` since it won't help us grasp what is the underlying principles of implementing an algorithm because it is an out-of-the-box (hence, ready-made) solution. We want to do it the hard way, not the easy way.\n",
    "\n",
    "Moreover, do notice that we can use `sklearn` package (or other packages) to make use of its useful functions, such as loading a dataset, as long as we don't use its already implemented algorithm models.\n",
    "\n",
    "We will be using:\n",
    "* `numpy` (obviously) to do all of the vectorized numerical computations on the dataset including the implementation of the algorithm,\n",
    "* `matplotlib` to plot graphs for better understanding the problem at hand with some visual aid,\n",
    "*` sklearn.datasets` to load some toy datasets to play around with our written code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m1kbVOE_PzjS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in our dataset is: 506\n"
     ]
    }
   ],
   "source": [
    "dataset = load_boston()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target[:,np.newaxis]\n",
    "\n",
    "print(\"Total samples in our dataset is: {}\".format(X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOs_op5TQeCE"
   },
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "Now, it's time to load the dataset we will be using throughout this post. The `sklearn.datasets` package offers some toy datasets to illustrate the behaviour of some algorithms and we will be using `load_boston()`function to return a regression dataset. Here, `dataset.data` represents the feature samples and `dataset.target` returns the target values, also called labels. \n",
    "\n",
    "It is important to note that, when we are loading the target values, we are adding a new dimension to the data (`dataset.target[:,np.newaxis]`), so that we can use the data as a column vector. Remember, linear algebra makes a distinction between row vectors and column vectors. However, in NumPy there are only n-dimensional arrays and no concept for row and column vectors, per se. We can use arrays of shape `(n, 1)` to imitate column vectors and `(1, n)` for row vectors. Ergo, we can use our target values of shape `(n, )` as a column vector of shape ` (n, 1)` by adding an axis explicitly. Luckily, we can do that with NumPy's own `newaxis` function which is used to increase the dimension of an array by one more dimension, when used once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, params):\n",
    "    n_samples = len(y)\n",
    "    h = X @ params\n",
    "    return (1/(2*n_samples))*np.sum((h-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "We have chosen the sum of squared errors (SSE) as our cost function, so we'll implement it here. h denotes our hypothesis function which is just a candidate function for our mapping from inputs (X) to outputs (y). When we take the inner product of our features with the parameters (`X @ params`), we are explicitly stating that we will be using linear regression for our hypthesis from a broad list of other machine learning algorithms, that is, we have decided that the relation between feature and target values is best described by the linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, params, learning_rate, n_iters):\n",
    "    n_samples = len(y)\n",
    "    J_history = np.zeros((n_iters,1))\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        params = params - (learning_rate/n_samples) * X.T @ (X @ params - y) \n",
    "        J_history[i] = compute_cost(X, y, params)\n",
    "\n",
    "    return (J_history, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "We can now implement gradient descent algorithm. Here, `n_iters` denotes the number of iterations for the gradient descent. We want to keep the history of our costs returned by the cost function in each iteration so we use an NumPy array `J_history` for that. As for the update rule, `1/n_samples) * X.T @ (X @ params - y) ` corresponds to the partial derivative of the cost function with respect to our parameters. So, `params` holds the updated parameters values according to the update rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = len(y)\n",
    "\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "\n",
    "X = (X-mu) / sigma\n",
    "\n",
    "X = np.hstack((np.ones((n_samples,1)),X))\n",
    "n_features = np.size(X,1)\n",
    "params = np.zeros((n_features,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "Before we run the gradient descent algorithm on our dataset, we normalize the data. Normalization is a technique often applied as part of data preparation in machine learning pipeline which typically means rescaling the values into a range of `[0,1]` to boost our accuracy hence lower the cost (error). Also, note that we initialize the paramaters (`params`) as zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "colab_type": "code",
    "id": "hogIdjr-f0s_",
    "outputId": "a0bcdd8d-65b7-48e2-eb0c-347cd92610d4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost is:  296.0734584980237 \n",
      "\n",
      "Optimal parameters are: \n",
      " [[22.53279993]\n",
      " [-0.83980839]\n",
      " [ 0.92612237]\n",
      " [-0.17541988]\n",
      " [ 0.72676226]\n",
      " [-1.82369448]\n",
      " [ 2.78447498]\n",
      " [-0.05650494]\n",
      " [-2.96695543]\n",
      " [ 1.80785186]\n",
      " [-1.1802415 ]\n",
      " [-1.99990382]\n",
      " [ 0.85595908]\n",
      " [-3.69524414]] \n",
      "\n",
      "Final cost is:  [11.00713381]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_iters = 1500\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, y, params)\n",
    "\n",
    "print(\"Initial cost is: \", initial_cost, \"\\n\")\n",
    "\n",
    "(J_history, optimal_params) = gradient_descent(X, y, params, learning_rate, n_iters)\n",
    "\n",
    "print(\"Optimal parameters are: \\n\", optimal_params, \"\\n\")\n",
    "\n",
    "print(\"Final cost is: \", J_history[-1])\n",
    "\n",
    "plt.plot(range(len(J_history)), J_history, 'r')\n",
    "\n",
    "plt.title(\"Convergence Graph of Cost Function\")\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-32m9tDgJ0Q"
   },
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "There you have it! We have run the algorithm successfully as we can clearly see that the cost decreased drastically from 296 to 11. The `gradient_descent` function returned the optimal parameter values, hence, we can now use them to predict new target values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "## Class Implementation for Linear Regression\n",
    "---\n",
    "\n",
    "Finally, after implementing linear regression from scratch we can rearrange the code we have written so far, add new code, make some modifications and turn it into a class implementation so that we have our very own linear regression package! There you go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearRegression():\n",
    "    def __init__(self, X, y, alpha=0.03, n_iter=1500):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.n_iter = n_iter\n",
    "        self.n_samples = len(y)\n",
    "        self.n_features = np.size(X, 1)\n",
    "        self.X = np.hstack((np.ones(\n",
    "            (self.n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n",
    "        self.y = y[:, np.newaxis]\n",
    "        self.params = np.zeros((self.n_features + 1, 1))\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            self.params = self.params - (self.alpha/self.n_samples) * \\\n",
    "            self.X.T @ (self.X @ self.params - self.y)\n",
    "\n",
    "        self.intercept_ = self.params[0]\n",
    "        self.coef_ = self.params[1:]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def score(self, X=None, y=None):\n",
    "\n",
    "        if X is None:\n",
    "            X = self.X\n",
    "        else:\n",
    "            n_samples = np.size(X, 0)\n",
    "            X = np.hstack((np.ones(\n",
    "                (n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))\n",
    "\n",
    "        if y is None:\n",
    "            y = self.y\n",
    "        else:\n",
    "            y = y[:, np.newaxis]\n",
    "\n",
    "        y_pred = X @ self.params\n",
    "        score = 1 - (((y - y_pred)**2).sum() / ((y - y.mean())**2).sum())\n",
    "\n",
    "        return score\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = np.size(X, 0)\n",
    "        y = np.hstack((np.ones((n_samples, 1)), (X-np.mean(X, 0)) \\\n",
    "                            / np.std(X, 0))) @ self.params\n",
    "        return y\n",
    "\n",
    "    def get_params(self):\n",
    "\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "Do notice the similarities between our implementation and sklearn's own implemenation of linear regression. I have done it on purpose of course, to show you that we can write a simplified version of a widely used package that works in a similar way to sklearn's implementation. I also (mostly) did it for the fun of it, though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "## Comparing Our Implementation with Sklearn's Linear Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Our Implementation  Sklearn's Implementation\n",
      "Training Accuracy            0.743482                  0.743500\n",
      "Test Accuracy                0.675347                  0.711226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import linear_regression as lr\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = load_boston()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "our_regressor = lr.LinearRegression(X_train, y_train).fit()\n",
    "sklearn_regressor = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "our_train_accuracy = our_regressor.score()\n",
    "sklearn_train_accuracy = sklearn_regressor.score(X_train, y_train)\n",
    "\n",
    "our_test_accuracy = our_regressor.score(X_test, y_test)\n",
    "sklearn_test_accuracy = sklearn_regressor.score(X_test, y_test)\n",
    "\n",
    "df_comparison = pd.DataFrame([[our_train_accuracy, sklearn_train_accuracy],[our_test_accuracy, sklearn_test_accuracy]],\n",
    "             ['Training Accuracy', 'Test Accuracy'],    \n",
    "             ['Our Implementation', 'Sklearn\\'s Implementation'])\n",
    "\n",
    "print(df_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Helvetica ;font-size: 14px; line-height:2.2\">\n",
    "\n",
    "We have done a pretty good job with that implementation, haven't we? Our training accuracy is almost the same as the sklearn's accuracy. Also, test accuracy is not so bad comparing to the test accuracy of sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear_Regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
