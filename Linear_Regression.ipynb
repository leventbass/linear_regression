{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sYCHsIVl-F1"
   },
   "source": [
    "# Linear Regression From Scratch With NumPy\n",
    "\n",
    "Welcome to the first post of the [Implementing Machine Learning Algorithms with Numpy](https://) series in which I'll try to show how one can implement some supervised, unsupervised and semi-supervised algorithms only with numpy package. \n",
    "\n",
    "Of course, we will use other useful packages such as `matplotlib`, ` seaborn` and etc. However, the use of other packages may only be limited to data visualization, data manipulation and/or loading datasets (e.g. `sklearn.datasets`) such that we won't take any shortcuts while writing the actual code for machine learning models.\n",
    "\n",
    "To sum it up, we will be implementing machine learning algorithms from scratch! Isn't that exciting and little bit overwhelming at the same time? Did I mention that it is super fun as well? The first algorithm that we will tackle is linear regression. Since it is the \"hello world\" algorithm of the machine learning universe, it will be pretty easy to implement it with NumPy. Let's start right away!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Qh7SBCLFy07"
   },
   "source": [
    "## Linear Regression Intuition\n",
    "\n",
    "<span style=\"font-family: Georgia, serif; font-size: 15px; letter-spacing: 2px;word-spacing: 5px;\">\n",
    "\n",
    "Before we write the code for implementation of linear regression, first we need to understand what exactly linear regression is. There are many useful resources out there that makes it quite easy to understand the concept behind regression and particularly linear regression so, I won't be going into much detail here. \n",
    "\n",
    "Linear regression is used to make some sense of the data we have at hand by unearthing the relation between target values and features. When we know this relation, we can make predictions about the data that we haven't seen before, in other words, we can infer the target value from feature values. Let's exemplify this: \n",
    "\n",
    "Suppose we want to understand how a company X decides what to pay to its employees. There may be so many factors that go into that decision and we go around and ask most of the employees who work there. After a lot of prying and sneaking around, it turns out that some of them earn a lot because they have been working at the company X for quite some time, some of them earn higher than most simply because they get along really well with the boss. Some earn higher because of their qualifications and talent. These three indicators seem to be the major ones. Now, with the information we have gathered, we want to understand the underlying relation between these factors and the salary that is paid to the employees currently. We come up with this oversimpflied equation:\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "$\\textbf{SALARY = } \\textrm{(? } \\times \\textbf{ Qualifications} \\textrm{) } + \\textrm{(? } \\times \\textbf{ Length of Service} \\textrm{) } + \\textrm{(? } \\times \\textbf{ Buttering up the Boss} \\textrm{) }$\n",
    "\n",
    "\n",
    "We can see from the equation above that the salary is affected by the 3 chosen attributes. These attributes, also called features, affect the salary according to their own weight which is depicted in the equation as question marks simply because we don't actually know what these weights are. \n",
    "\n",
    "Now, let's imagine what would happen if we know these weights exactly. Then if we have an employee whose salary we don't know, we can use her features (qualifications, length of service etc.) to predict her salary. That is, we would understand how these features and the target value (salary) are related. \n",
    "\n",
    "Turns out, linear regression is used to do exactly that! It is used to get a good estimate of these weights so that they can be used to predict the target value of unseen data. In machine learning literature these weights are often called parameters, hence from now on, we'll adopt that term here as well. \n",
    "\n",
    "## Gradient Descent Algorithm\n",
    "\n",
    "Now that we know \"what\" linear regression is, we can come to the \"how\" part. How does this algorithm work? How can we figure out these parameters for linear regression? In machine learning, there is another famous algorithm called \"gradient descent\" that is used a lot, not only for estimating the parameters for linear regression but for other optimization problems as well. In gradient descent algorithm, parameters of the model (here, that is linear regression) is changed iteratively at each step starting with the initial values of the parameters. \n",
    "\n",
    "To remind us once more, parameters (weights) are the numerical values that determine how much each feature affects the target value. We want to know these parameter values exactly, but in real life this is not possible because there may be so many other features (hence, parameters of those features as well) affecting the target value. However, we want them to predict the target value as close as possible to the actual value.  Since, the question marks in the above equation represent the parameter values, we can replace them with initial values like this:\n",
    "\n",
    "$\\textbf{SALARY = } \\textrm{(1000 } \\times \\textbf{ Qualifications} \\textrm{) } + \\textrm{(200 } \\times \\textbf{ Length of Service} \\textrm{) } + \\textrm{(500 } \\times \\textbf{ Buttering up the Boss} \\textrm{) }$\n",
    "\n",
    "**SALARY** = (1000 x _Qualifications_) + (200 x _Length of Service_) + (500 x _Buttering up the Boss_)\n",
    "\n",
    "<span style=\"color: #f2cf4a; font-family: Babas; font-size: 2em;\">INSPIRATION DAY</span>\n",
    "\n",
    "Here, it is obvious that qualifications feature affects salary more than the other features, because its parameter value is higher than the rest. Keep in mind that we have chosen these parameter values intuitively and we will be using them as our initial parameter values, yet these initial values will change at every step of the algorithm towards their optimal values.\n",
    "\n",
    "Going along with our analogy, suppose we have an initial estimate for the parameters of these features and we went around and asked these questions to the first employee we could find:\n",
    "\n",
    " 1. For how long have you been working here?\n",
    " 2. What are your qualifications for your position?\n",
    " 3. How do you get along with your boss? (Does your boss seem to like or dislike you?) \n",
    "\n",
    "For the first question, we told the employee that we would accept an answer in years (1 year, 2 years, 5 years etc.). For the second question, we told the employee that the answer would be any number from 1 to 10 (1 being the least qualified and 10 the most). For the last question, the answer would be a number from -5 to 5. Here, minus represents the negativity of the relationship between the employee and the boss. Therefore, -5 means that the boss quite dislikes the employee, 0 could mean that the boss doesn't even know the employee and/or there is no interaction between the two and +5 means that two of them get along just great. \n",
    "\n",
    "When we asked the employee these questions, these are the answers we got:\n",
    "1. I've have been working here for 10 years.\n",
    "2. I can honestly say that I'm overqualified for this job. So I would give it a 9.\n",
    "3. My boss seems to hate me. Whenever I'm around I can see the hatred in his eyes. So I would give it a -4.\n",
    "\n",
    "Remember that we want to predict the salary based the parameters we have chosen and the answers we've got from the employee. After predicting what the salary would be based on only these answers, we ask the employee what the actual salary is. The difference between the predicted and actual value determines how successful our estimates for these parameters (weights) are. Gradient descent algorithm's job would be to make this difference (predicted - actual) as small as possible. Let's go ahead and call this difference \"error\", since it represents how much off the actual value is from the predicted value. Now, let's plug the numbers we get from the first employee's answers into our equation:\n",
    "\n",
    "$\\textbf{SALARY = } \\textrm{(1000 } \\times \\textbf{ 10} \\textrm{) } + \\textrm{(200 } \\times \\textbf{ 9} \\textrm{) } + \\textrm{(500 } \\times \\textbf{ -4} \\textrm{) }$\n",
    "\n",
    "Hence, this shows that our prediction for the salary is:\n",
    "\n",
    "$\\textbf{SALARY}_\\textrm{predicted} = 12800 $\n",
    "\n",
    "\n",
    "Now, we ask the employee what her actual salary is and we calculate the error between the actual and predicted value:\n",
    "\n",
    "$\\textbf{SALARY}_\\textrm{actual} = 9800 $\n",
    "\n",
    "$\\textbf{Error } = \\textbf{SALARY}_\\textrm{predicted} - \\textbf{SALARY}_\\textrm{actual} = 12800 - 9800 = 3000 $\n",
    "\n",
    "\n",
    "We see that our error is 3000 which is a lot and we have to make this error as small as possible by tweaking the parameter values appropriately. But how do we do that? How can we decide what is the correct way of changing the parameter values? Obviously, we can make guesses intuitively and change the parameter values (increase or decrease) to make the error small enough. However, this won't be very easy if we have 100 features and not only three. 100 features mean 100 parameter values, remember. Obviously, we have to find a better way than this. Moreover, there is another factor to consider. This error cannot represent only one employee, in other words, we cannot only change the parameter values for one employee since we want this model to be representative for all the employees who work at company X. We have to get the answers from all the employees and plug those numbers into the equation, find the error and change the parameters accordingly. \n",
    "\n",
    "Perhaps we could go over all of the employees at company X and sum all of the individual errors, just like this:\n",
    "\n",
    "$Total Error = \\left( \\sum_{k=1}^n Error_k \\right)^2 $ , where n is the total number of employees who work at company X.\n",
    "\n",
    "The error function that we have used here (Error = Predicted - Actual) is one of the most basic function to be used in machine learning pipeline, and often it has its certain limitationsso now let's use a more adopted version which is called \"ordinary least squares\" which is simply the the sum of square differences between the actual and predicted values:\n",
    "\n",
    "Ordinary Least Squares = \n",
    "\n",
    "Now, a quick change of notation is in order: Cost function is often used a lot instead of error. Because, it basically costs us to miss the actual value by a value of predicted-actual. If predicted value was equal to the actual value, the cost would be zero. Therefore, a cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between feature values and target values. ----> add this to the error part.\n",
    "\n",
    "After establishing the cost function we can now move on. The whole point of gradient descent algorithm is to minimize the cost function. When we minimize the cost function, we are actually guaranteeing the possible lowest error while increasing the accuracy of our model. We go over our dataset iteratively while updating the parameters at each step. Back to our analogy, remember that we had three parameters (qualifications, length of service, buttering up the boss) that we wanted to change in the direction that minimized the cost function. So checking each data point in our dataset basically means asking each employee who works at company X those 3 questions we convised and using the answers and plugging those numbers into the cost function hence, calculating the cost function and deciding which direction the next step we should take to minimize the cost function.\n",
    "\n",
    "Now, how do we decide which direction we should go to make the total cost a bit smaller? Calculus comes to our help here. Hence, when we want to minimize a function we take the derivative of the function with respect to a variable and use that derivative to decide which direction to go. In our analogy, the parameters that we have chosen are actually the variables of our cost function because the cost function varies as each parameter varies (variable, duh). We have to take the derivative with respect to each parameter (cough, variable) and update the parameters using those derivative values. In the picture below, we can see the graph of the cost function against just one parameter (Length of Service). Now when we calculate the partial derivative of the cost function with respect to this parameter only, we get the direction we need to move towards for this parameter, in order to reach the local minima whose slope equals to 0.\n",
    "\n",
    "<img src=\"img/cost_function.png\" width=400 height=200 > <br> <br>\n",
    "\n",
    "When we take the derivative with respect to each parameter and find the direction we need to move towards, we update each parameter simultaneously:\n",
    "\n",
    "$\n",
    "\\textit{Length of Service}_\\textrm{updated} = \\textit{Length of Service}_\\textrm{old} - \\textbf{ (Learning Rate} \\times \\textit{Partial Derivative w.r.t. Length of Service)} \n",
    "$\n",
    "\n",
    "This update rule is applied to all of the parameters using their partial derivatives correspondingly. Here, learning rate as it is also called learning step, is the amount that the parameters are updated during learning the optimized parameter values. Learning rate is a configurable hyperparameter, often in the range between 0.0 and 1.0, that controls the rate or speed at which the model learns. If it's high, the model learns quickly, however it's too much high the during the update we might miss the optimal value because we took a really big step. If the learning rate is too low, then the model will take a lot of time to converge to the lowest cost function value. \n",
    "\n",
    "So one iteration means asking all of the employees for once (or going over the dataset for once). After one iteration, we update the parameter values accordingly (We'll get to that later). Suppose :\n",
    "\n",
    "Total cost we get after 1st iteration: 123444\n",
    "Total cost after 2nd iteration: 88283\n",
    "Total cost after 3rd iteration: 2234\n",
    "....\n",
    "Total cost after 100th iteration: 541\n",
    "\n",
    "\n",
    "So one iteration means asking all of the employees for once (or going over the dataset for once) and updating the parameter values accordingly. After going through the dataset many times, iterating stops when we reach a point where the cost is low enough for us to decide that we can stop the algorithm and use the parameter values that were updated up until now. Then, we can use those \"opitimized\" values to predict new target values for new feature values. And those predictions will be pretty good. What do we mean by \"optimized\" here? Well, now we have found parameter values for our 3 features, so that they can predict new target values with possible lowest error. Hence, we optimized those parameters in our model. This is where learning of the \"machine learning\" happens indeed. We \"learn\" the parameters that minimizes our cost function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fOVZo_QaP4eX"
   },
   "source": [
    "## 1. Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-X9lP0wwR5u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3_6cl5YwUMw"
   },
   "source": [
    "First things first, we start by importing necessary libraries to help us along the way. As I have mentioned before, we won't be using any packages that will give us already implemented algorithm models such as `sklearn.linear_model` since it won't help us grasp what is the underlying principles of implementing an algorithm because it is an out-of-the-box (hence, ready-made) solution. We want to do it the hard way, not the easy way.\n",
    "\n",
    "Moreover, do notice that we can use `sklearn` package (or other packages) to make use of its useful functions, such as loading a dataset, as long as we don't use its already implemented algorithm models.\n",
    "\n",
    "We will be using:\n",
    "* `numpy` (obviously) to do all of the vectorized numerical computations on the dataset including the implementation of the algorithm,\n",
    "* `matplotlib` to plot graphs for better understanding the problem at hand with some visual aid,\n",
    "*` sklearn.datasets` to load some toy datasets to play around with our written code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xIlVnTXRQQDU"
   },
   "source": [
    "### 2. Loading and exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m1kbVOE_PzjS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in our dataset is: 506\n"
     ]
    }
   ],
   "source": [
    "dataset = load_boston()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target[:,np.newaxis]\n",
    "\n",
    "print(\"Total samples in our dataset is: {}\".format(X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOs_op5TQeCE"
   },
   "source": [
    "Now, it's time to load the dataset we will be using throughout this post. The `sklearn.datasets` package offers some toy datasets to illustrate the behaviour of some algorithms and we will be using `load_boston()`function to return a regression dataset. Here, `dataset.data` represents the feature samples and `dataset.target` returns the label values. \n",
    "\n",
    "It is important to note that, when we are loading the target values, we are adding a new dimension to the data (`dataset.target[:,np.newaxis]`), so that we can use the data as a column vector. Remember, linear algebra makes a distinction between row vectors and column vectors. However, in NumPy there are only n-dimensional arrays and no concept for row and column vectors, per se. We can use arrays of shape `(n, 1)` to imitate column vectors and `(1, n)` for row vectors. Ergo, we can use our target values of shape `(n, )` as a column vector of shape ` (n, 1)` by adding an axis explicitly. Luckily, we can do that with NumPy's own `newaxis` function which is used to increase the dimension of an array by one more dimension, when used once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "colab_type": "code",
    "id": "hogIdjr-f0s_",
    "outputId": "a0bcdd8d-65b7-48e2-eb0c-347cd92610d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 296.0734584980237\n",
      "Optimal Theta is:  [[ 2.25328063e+01]\n",
      " [-9.28135085e-01]\n",
      " [ 1.08154929e+00]\n",
      " [ 1.40840005e-01]\n",
      " [ 6.81748308e-01]\n",
      " [-2.05670784e+00]\n",
      " [ 2.67424105e+00]\n",
      " [ 1.94568604e-02]\n",
      " [-3.10404863e+00]\n",
      " [ 2.66206628e+00]\n",
      " [-2.07660959e+00]\n",
      " [-2.06060107e+00]\n",
      " [ 8.49267351e-01]\n",
      " [-3.74362129e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    J_history = np.zeros((iterations,1))\n",
    "\n",
    "    for i in range(iterations):\n",
    "        theta = theta - (alpha/m) * X.T @ (X @ theta - y) \n",
    "        J_history[i] = compute_cost(X, y, theta)\n",
    "\n",
    "    return (J_history, theta)\n",
    "\n",
    "\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = X @ theta\n",
    "    return (1/(2*m))*np.sum((h-y)**2)\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_boston()\n",
    "X = dataset.data\n",
    "y = dataset.target[:,np.newaxis]\n",
    "\n",
    "m = len(y)\n",
    "\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "\n",
    "X = (X-mu) / sigma\n",
    "\n",
    "X = np.hstack((np.ones((m,1)),X))\n",
    "n = np.size(X,1)\n",
    "theta = np.zeros((n,1))\n",
    "\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "\n",
    "InitialCost = compute_cost(X, y, theta)\n",
    "\n",
    "print(\"Initial Cost is: {}\".format(InitialCost))\n",
    "\n",
    "(J_history, theta_optimal) = gradient_descent(X, y, theta, alpha, iterations)\n",
    "\n",
    "(J_history_2, theta_optimal) = gradient_descent(X, y, theta, 0.001, iterations)\n",
    "\n",
    "(J_history_3, theta_optimal) = gradient_descent(X, y, theta, 0.1, iterations)\n",
    "\n",
    "print(\"Optimal Theta is: \", theta_optimal)\n",
    "\n",
    "plt.plot(range(len(J_history)), J_history, 'r')\n",
    "plt.plot(range(len(J_history_2)), J_history_2, 'y')\n",
    "plt.plot(range(len(J_history_3)), J_history_3, 'g')\n",
    "plt.title(\"Convergence Graph of Cost Function\")\n",
    "plt.legend((\"alpha: 0.01\", \"alpha: 0.001\", \"alpha: 0.1\"))\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-32m9tDgJ0Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear_Regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
